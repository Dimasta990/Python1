{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston=load_boston()\n",
    "boston.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=boston.data\n",
    "target=boston.target\n",
    "feature_names=boston.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pd.DataFrame(data,columns=feature_names)\n",
    "y=pd.DataFrame(target,columns=['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=LinearRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "y_pred=lr.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7112260057484925"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2=r2_score (y_test,y_pred)\n",
    "r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFR=RandomForestRegressor(n_estimators=1000, max_depth=12, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8749965273218174"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RFR.fit(X_train,y_train.values[:, 0])\n",
    "y_pred=RFR.predict(x_test)\n",
    "r2=r2_score (y_test,y_pred)\n",
    "r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предсказательная способность второй модели выше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class RandomForestRegressor in module sklearn.ensemble.forest:\n",
      "\n",
      "class RandomForestRegressor(ForestRegressor)\n",
      " |  RandomForestRegressor(n_estimators='warn', criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False)\n",
      " |  \n",
      " |  A random forest regressor.\n",
      " |  \n",
      " |  A random forest is a meta estimator that fits a number of classifying\n",
      " |  decision trees on various sub-samples of the dataset and uses averaging\n",
      " |  to improve the predictive accuracy and control over-fitting.\n",
      " |  The sub-sample size is always the same as the original\n",
      " |  input sample size but the samples are drawn with replacement if\n",
      " |  `bootstrap=True` (default).\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <forest>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_estimators : integer, optional (default=10)\n",
      " |      The number of trees in the forest.\n",
      " |  \n",
      " |      .. versionchanged:: 0.20\n",
      " |         The default value of ``n_estimators`` will change from 10 in\n",
      " |         version 0.20 to 100 in version 0.22.\n",
      " |  \n",
      " |  criterion : string, optional (default=\"mse\")\n",
      " |      The function to measure the quality of a split. Supported criteria\n",
      " |      are \"mse\" for the mean squared error, which is equal to variance\n",
      " |      reduction as feature selection criterion, and \"mae\" for the mean\n",
      " |      absolute error.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |         Mean Absolute Error (MAE) criterion.\n",
      " |  \n",
      " |  max_depth : integer or None, optional (default=None)\n",
      " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      " |      all leaves are pure or until all leaves contain less than\n",
      " |      min_samples_split samples.\n",
      " |  \n",
      " |  min_samples_split : int, float, optional (default=2)\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a fraction and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_samples_leaf : int, float, optional (default=1)\n",
      " |      The minimum number of samples required to be at a leaf node.\n",
      " |      A split point at any depth will only be considered if it leaves at\n",
      " |      least ``min_samples_leaf`` training samples in each of the left and\n",
      " |      right branches.  This may have the effect of smoothing the model,\n",
      " |      especially in regression.\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a fraction and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, optional (default=0.)\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_features : int, float, string or None, optional (default=\"auto\")\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |      - If int, then consider `max_features` features at each split.\n",
      " |      - If float, then `max_features` is a fraction and\n",
      " |        `int(max_features * n_features)` features are considered at each\n",
      " |        split.\n",
      " |      - If \"auto\", then `max_features=n_features`.\n",
      " |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      " |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |      - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  max_leaf_nodes : int or None, optional (default=None)\n",
      " |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  min_impurity_decrease : float, optional (default=0.)\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  min_impurity_split : float, (default=1e-7)\n",
      " |      Threshold for early stopping in tree growth. A node will split\n",
      " |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      " |  \n",
      " |      .. deprecated:: 0.19\n",
      " |         ``min_impurity_split`` has been deprecated in favor of\n",
      " |         ``min_impurity_decrease`` in 0.19. The default value of\n",
      " |         ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n",
      " |         will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n",
      " |  \n",
      " |  bootstrap : boolean, optional (default=True)\n",
      " |      Whether bootstrap samples are used when building trees. If False, the\n",
      " |      whole datset is used to build each tree.\n",
      " |  \n",
      " |  oob_score : bool, optional (default=False)\n",
      " |      whether to use out-of-bag samples to estimate\n",
      " |      the R^2 on unseen data.\n",
      " |  \n",
      " |  n_jobs : int or None, optional (default=None)\n",
      " |      The number of jobs to run in parallel for both `fit` and `predict`.\n",
      " |      `None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      " |      for more details.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional (default=None)\n",
      " |      If int, random_state is the seed used by the random number generator;\n",
      " |      If RandomState instance, random_state is the random number generator;\n",
      " |      If None, the random number generator is the RandomState instance used\n",
      " |      by `np.random`.\n",
      " |  \n",
      " |  verbose : int, optional (default=0)\n",
      " |      Controls the verbosity when fitting and predicting.\n",
      " |  \n",
      " |  warm_start : bool, optional (default=False)\n",
      " |      When set to ``True``, reuse the solution of the previous call to fit\n",
      " |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      " |      new forest. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  estimators_ : list of DecisionTreeRegressor\n",
      " |      The collection of fitted sub-estimators.\n",
      " |  \n",
      " |  feature_importances_ : array of shape = [n_features]\n",
      " |      The feature importances (the higher, the more important the feature).\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of features when ``fit`` is performed.\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      The number of outputs when ``fit`` is performed.\n",
      " |  \n",
      " |  oob_score_ : float\n",
      " |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      " |  \n",
      " |  oob_prediction_ : array of shape = [n_samples]\n",
      " |      Prediction computed with out-of-bag estimate on the training set.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.ensemble import RandomForestRegressor\n",
      " |  >>> from sklearn.datasets import make_regression\n",
      " |  \n",
      " |  >>> X, y = make_regression(n_features=4, n_informative=2,\n",
      " |  ...                        random_state=0, shuffle=False)\n",
      " |  >>> regr = RandomForestRegressor(max_depth=2, random_state=0,\n",
      " |  ...                              n_estimators=100)\n",
      " |  >>> regr.fit(X, y)  # doctest: +NORMALIZE_WHITESPACE\n",
      " |  RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=2,\n",
      " |             max_features='auto', max_leaf_nodes=None,\n",
      " |             min_impurity_decrease=0.0, min_impurity_split=None,\n",
      " |             min_samples_leaf=1, min_samples_split=2,\n",
      " |             min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
      " |             oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
      " |  >>> print(regr.feature_importances_)\n",
      " |  [0.18146984 0.81473937 0.00145312 0.00233767]\n",
      " |  >>> print(regr.predict([[0, 0, 0, 0]]))\n",
      " |  [-8.32987858]\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The default values for the parameters controlling the size of the trees\n",
      " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      " |  unpruned trees which can potentially be very large on some data sets. To\n",
      " |  reduce memory consumption, the complexity and size of the trees should be\n",
      " |  controlled by setting those parameter values.\n",
      " |  \n",
      " |  The features are always randomly permuted at each split. Therefore,\n",
      " |  the best found split may vary, even with the same training data,\n",
      " |  ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
      " |  of the criterion is identical for several splits enumerated during the\n",
      " |  search of the best split. To obtain a deterministic behaviour during\n",
      " |  fitting, ``random_state`` has to be fixed.\n",
      " |  \n",
      " |  The default value ``max_features=\"auto\"`` uses ``n_features``\n",
      " |  rather than ``n_features / 3``. The latter was originally suggested in\n",
      " |  [1], whereas the former was more recently justified empirically in [2].\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      " |  \n",
      " |  .. [2] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\n",
      " |         trees\", Machine Learning, 63(1), 3-42, 2006.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  DecisionTreeRegressor, ExtraTreesRegressor\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RandomForestRegressor\n",
      " |      ForestRegressor\n",
      " |      BaseForest\n",
      " |      sklearn.ensemble.base.BaseEnsemble\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_estimators='warn', criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from ForestRegressor:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict regression target for X.\n",
      " |      \n",
      " |      The predicted regression target of an input sample is computed as the\n",
      " |      mean predicted regression targets of the trees in the forest.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array of shape = [n_samples] or [n_samples, n_outputs]\n",
      " |          The predicted values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseForest:\n",
      " |  \n",
      " |  apply(self, X)\n",
      " |      Apply trees in the forest to X, return leaf indices.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array_like, shape = [n_samples, n_estimators]\n",
      " |          For each datapoint x in X and for each tree in the forest,\n",
      " |          return the index of the leaf x ends up in.\n",
      " |  \n",
      " |  decision_path(self, X)\n",
      " |      Return the decision path in the forest\n",
      " |      \n",
      " |      .. versionadded:: 0.18\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      indicator : sparse csr array, shape = [n_samples, n_nodes]\n",
      " |          Return a node indicator matrix where non zero elements\n",
      " |          indicates that the samples goes through the nodes.\n",
      " |      \n",
      " |      n_nodes_ptr : array of size (n_estimators + 1, )\n",
      " |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      " |          gives the indicator value for the i-th estimator.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Build a forest of trees from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The training input samples. Internally, its dtype will be converted\n",
      " |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like, shape = [n_samples] or [n_samples, n_outputs]\n",
      " |          The target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples] or None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. In the case of\n",
      " |          classification, splits are also ignored if they would result in any\n",
      " |          single class carrying a negative weight in either child node.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseForest:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Return the feature importances (the higher, the more important the\n",
      " |         feature).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : array, shape = [n_features]\n",
      " |          The values of this array sum to 1, unless all trees are single node\n",
      " |          trees consisting of only the root node, in which case it will be an\n",
      " |          array of zeros.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble.base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Returns the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Returns iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Returns the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the coefficient of determination R^2 of the prediction.\n",
      " |      \n",
      " |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      " |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      " |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always\n",
      " |      predicts the expected value of y, disregarding the input features,\n",
      " |      would get a R^2 score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples. For some estimators this may be a\n",
      " |          precomputed kernel matrix instead, shape = (n_samples,\n",
      " |          n_samples_fitted], where n_samples_fitted is the number of\n",
      " |          samples used in the fitting for the estimator.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True values for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          R^2 of self.predict(X) wrt. y.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The R2 score used when calling ``score`` on a regressor will use\n",
      " |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      " |      with `metrics.r2_score`. This will influence the ``score`` method of\n",
      " |      all the multioutput regressors (except for\n",
      " |      `multioutput.MultiOutputRegressor`). To specify the default value\n",
      " |      manually and avoid the warning, please either call `metrics.r2_score`\n",
      " |      directly or make a custom scorer with `metrics.make_scorer` (the\n",
      " |      built-in scorer ``'r2'`` uses ``multioutput='uniform_average'``).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(RandomForestRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999999"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances_sum=RFR.feature_importances_.sum()\n",
    "importances_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
       "       'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03211748, 0.00154999, 0.0070941 , 0.0011488 , 0.01436832,\n",
       "       0.40270459, 0.01424477, 0.06403265, 0.00496762, 0.01169177,\n",
       "       0.01808961, 0.0123114 , 0.41567892])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "importances = RFR.feature_importances_\n",
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 13 artists>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.bar(feature_names,importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наибольшую важность показывают:\n",
    "LSTAT % lower status of the population\n",
    "RM average number of rooms per dwelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      "Time      284807 non-null float64\n",
      "V1        284807 non-null float64\n",
      "V2        284807 non-null float64\n",
      "V3        284807 non-null float64\n",
      "V4        284807 non-null float64\n",
      "V5        284807 non-null float64\n",
      "V6        284807 non-null float64\n",
      "V7        284807 non-null float64\n",
      "V8        284807 non-null float64\n",
      "V9        284807 non-null float64\n",
      "V10       284807 non-null float64\n",
      "V11       284807 non-null float64\n",
      "V12       284807 non-null float64\n",
      "V13       284807 non-null float64\n",
      "V14       284807 non-null float64\n",
      "V15       284807 non-null float64\n",
      "V16       284807 non-null float64\n",
      "V17       284807 non-null float64\n",
      "V18       284807 non-null float64\n",
      "V19       284807 non-null float64\n",
      "V20       284807 non-null float64\n",
      "V21       284807 non-null float64\n",
      "V22       284807 non-null float64\n",
      "V23       284807 non-null float64\n",
      "V24       284807 non-null float64\n",
      "V25       284807 non-null float64\n",
      "V26       284807 non-null float64\n",
      "V27       284807 non-null float64\n",
      "V28       284807 non-null float64\n",
      "Amount    284807 non-null float64\n",
      "Class     284807 non-null int64\n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "cwd\n",
    "os.chdir(\"C:/Users/Dimas/Desktop/Geekbrains/Python\")\n",
    "os.listdir('.')\n",
    "df=pd.read_csv('creditcard.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>-0.551600</td>\n",
       "      <td>-0.617801</td>\n",
       "      <td>-0.991390</td>\n",
       "      <td>-0.311169</td>\n",
       "      <td>1.468177</td>\n",
       "      <td>-0.470401</td>\n",
       "      <td>0.207971</td>\n",
       "      <td>0.025791</td>\n",
       "      <td>0.403993</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>1.612727</td>\n",
       "      <td>1.065235</td>\n",
       "      <td>0.489095</td>\n",
       "      <td>-0.143772</td>\n",
       "      <td>0.635558</td>\n",
       "      <td>0.463917</td>\n",
       "      <td>-0.114805</td>\n",
       "      <td>-0.183361</td>\n",
       "      <td>-0.145783</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>0.624501</td>\n",
       "      <td>0.066084</td>\n",
       "      <td>0.717293</td>\n",
       "      <td>-0.165946</td>\n",
       "      <td>2.345865</td>\n",
       "      <td>-2.890083</td>\n",
       "      <td>1.109969</td>\n",
       "      <td>-0.121359</td>\n",
       "      <td>-2.261857</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>-0.226487</td>\n",
       "      <td>0.178228</td>\n",
       "      <td>0.507757</td>\n",
       "      <td>-0.287924</td>\n",
       "      <td>-0.631418</td>\n",
       "      <td>-1.059647</td>\n",
       "      <td>-0.684093</td>\n",
       "      <td>1.965775</td>\n",
       "      <td>-1.232622</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>-0.822843</td>\n",
       "      <td>0.538196</td>\n",
       "      <td>1.345852</td>\n",
       "      <td>-1.119670</td>\n",
       "      <td>0.175121</td>\n",
       "      <td>-0.451449</td>\n",
       "      <td>-0.237033</td>\n",
       "      <td>-0.038195</td>\n",
       "      <td>0.803487</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.425966</td>\n",
       "      <td>0.960523</td>\n",
       "      <td>1.141109</td>\n",
       "      <td>-0.168252</td>\n",
       "      <td>0.420987</td>\n",
       "      <td>-0.029728</td>\n",
       "      <td>0.476201</td>\n",
       "      <td>0.260314</td>\n",
       "      <td>-0.568671</td>\n",
       "      <td>-0.371407</td>\n",
       "      <td>1.341262</td>\n",
       "      <td>0.359894</td>\n",
       "      <td>-0.358091</td>\n",
       "      <td>-0.137134</td>\n",
       "      <td>0.517617</td>\n",
       "      <td>0.401726</td>\n",
       "      <td>-0.058133</td>\n",
       "      <td>0.068653</td>\n",
       "      <td>-0.033194</td>\n",
       "      <td>0.084968</td>\n",
       "      <td>-0.208254</td>\n",
       "      <td>-0.559825</td>\n",
       "      <td>-0.026398</td>\n",
       "      <td>-0.371427</td>\n",
       "      <td>-0.232794</td>\n",
       "      <td>0.105915</td>\n",
       "      <td>0.253844</td>\n",
       "      <td>0.081080</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.229658</td>\n",
       "      <td>0.141004</td>\n",
       "      <td>0.045371</td>\n",
       "      <td>1.202613</td>\n",
       "      <td>0.191881</td>\n",
       "      <td>0.272708</td>\n",
       "      <td>-0.005159</td>\n",
       "      <td>0.081213</td>\n",
       "      <td>0.464960</td>\n",
       "      <td>-0.099254</td>\n",
       "      <td>-1.416907</td>\n",
       "      <td>-0.153826</td>\n",
       "      <td>-0.751063</td>\n",
       "      <td>0.167372</td>\n",
       "      <td>0.050144</td>\n",
       "      <td>-0.443587</td>\n",
       "      <td>0.002821</td>\n",
       "      <td>-0.611987</td>\n",
       "      <td>-0.045575</td>\n",
       "      <td>-0.219633</td>\n",
       "      <td>-0.167716</td>\n",
       "      <td>-0.270710</td>\n",
       "      <td>-0.154104</td>\n",
       "      <td>-0.780055</td>\n",
       "      <td>0.750137</td>\n",
       "      <td>-0.257237</td>\n",
       "      <td>0.034507</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>4.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.644269</td>\n",
       "      <td>1.417964</td>\n",
       "      <td>1.074380</td>\n",
       "      <td>-0.492199</td>\n",
       "      <td>0.948934</td>\n",
       "      <td>0.428118</td>\n",
       "      <td>1.120631</td>\n",
       "      <td>-3.807864</td>\n",
       "      <td>0.615375</td>\n",
       "      <td>1.249376</td>\n",
       "      <td>-0.619468</td>\n",
       "      <td>0.291474</td>\n",
       "      <td>1.757964</td>\n",
       "      <td>-1.323865</td>\n",
       "      <td>0.686133</td>\n",
       "      <td>-0.076127</td>\n",
       "      <td>-1.222127</td>\n",
       "      <td>-0.358222</td>\n",
       "      <td>0.324505</td>\n",
       "      <td>-0.156742</td>\n",
       "      <td>1.943465</td>\n",
       "      <td>-1.015455</td>\n",
       "      <td>0.057504</td>\n",
       "      <td>-0.649709</td>\n",
       "      <td>-0.415267</td>\n",
       "      <td>-0.051634</td>\n",
       "      <td>-1.206921</td>\n",
       "      <td>-1.085339</td>\n",
       "      <td>40.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.894286</td>\n",
       "      <td>0.286157</td>\n",
       "      <td>-0.113192</td>\n",
       "      <td>-0.271526</td>\n",
       "      <td>2.669599</td>\n",
       "      <td>3.721818</td>\n",
       "      <td>0.370145</td>\n",
       "      <td>0.851084</td>\n",
       "      <td>-0.392048</td>\n",
       "      <td>-0.410430</td>\n",
       "      <td>-0.705117</td>\n",
       "      <td>-0.110452</td>\n",
       "      <td>-0.286254</td>\n",
       "      <td>0.074355</td>\n",
       "      <td>-0.328783</td>\n",
       "      <td>-0.210077</td>\n",
       "      <td>-0.499768</td>\n",
       "      <td>0.118765</td>\n",
       "      <td>0.570328</td>\n",
       "      <td>0.052736</td>\n",
       "      <td>-0.073425</td>\n",
       "      <td>-0.268092</td>\n",
       "      <td>-0.204233</td>\n",
       "      <td>1.011592</td>\n",
       "      <td>0.373205</td>\n",
       "      <td>-0.384157</td>\n",
       "      <td>0.011747</td>\n",
       "      <td>0.142404</td>\n",
       "      <td>93.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.338262</td>\n",
       "      <td>1.119593</td>\n",
       "      <td>1.044367</td>\n",
       "      <td>-0.222187</td>\n",
       "      <td>0.499361</td>\n",
       "      <td>-0.246761</td>\n",
       "      <td>0.651583</td>\n",
       "      <td>0.069539</td>\n",
       "      <td>-0.736727</td>\n",
       "      <td>-0.366846</td>\n",
       "      <td>1.017614</td>\n",
       "      <td>0.836390</td>\n",
       "      <td>1.006844</td>\n",
       "      <td>-0.443523</td>\n",
       "      <td>0.150219</td>\n",
       "      <td>0.739453</td>\n",
       "      <td>-0.540980</td>\n",
       "      <td>0.476677</td>\n",
       "      <td>0.451773</td>\n",
       "      <td>0.203711</td>\n",
       "      <td>-0.246914</td>\n",
       "      <td>-0.633753</td>\n",
       "      <td>-0.120794</td>\n",
       "      <td>-0.385050</td>\n",
       "      <td>-0.069733</td>\n",
       "      <td>0.094199</td>\n",
       "      <td>0.246219</td>\n",
       "      <td>0.083076</td>\n",
       "      <td>3.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "5   2.0 -0.425966  0.960523  1.141109 -0.168252  0.420987 -0.029728  0.476201   \n",
       "6   4.0  1.229658  0.141004  0.045371  1.202613  0.191881  0.272708 -0.005159   \n",
       "7   7.0 -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118  1.120631   \n",
       "8   7.0 -0.894286  0.286157 -0.113192 -0.271526  2.669599  3.721818  0.370145   \n",
       "9   9.0 -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761  0.651583   \n",
       "\n",
       "         V8        V9       V10       V11       V12       V13       V14  \\\n",
       "0  0.098698  0.363787  0.090794 -0.551600 -0.617801 -0.991390 -0.311169   \n",
       "1  0.085102 -0.255425 -0.166974  1.612727  1.065235  0.489095 -0.143772   \n",
       "2  0.247676 -1.514654  0.207643  0.624501  0.066084  0.717293 -0.165946   \n",
       "3  0.377436 -1.387024 -0.054952 -0.226487  0.178228  0.507757 -0.287924   \n",
       "4 -0.270533  0.817739  0.753074 -0.822843  0.538196  1.345852 -1.119670   \n",
       "5  0.260314 -0.568671 -0.371407  1.341262  0.359894 -0.358091 -0.137134   \n",
       "6  0.081213  0.464960 -0.099254 -1.416907 -0.153826 -0.751063  0.167372   \n",
       "7 -3.807864  0.615375  1.249376 -0.619468  0.291474  1.757964 -1.323865   \n",
       "8  0.851084 -0.392048 -0.410430 -0.705117 -0.110452 -0.286254  0.074355   \n",
       "9  0.069539 -0.736727 -0.366846  1.017614  0.836390  1.006844 -0.443523   \n",
       "\n",
       "        V15       V16       V17       V18       V19       V20       V21  \\\n",
       "0  1.468177 -0.470401  0.207971  0.025791  0.403993  0.251412 -0.018307   \n",
       "1  0.635558  0.463917 -0.114805 -0.183361 -0.145783 -0.069083 -0.225775   \n",
       "2  2.345865 -2.890083  1.109969 -0.121359 -2.261857  0.524980  0.247998   \n",
       "3 -0.631418 -1.059647 -0.684093  1.965775 -1.232622 -0.208038 -0.108300   \n",
       "4  0.175121 -0.451449 -0.237033 -0.038195  0.803487  0.408542 -0.009431   \n",
       "5  0.517617  0.401726 -0.058133  0.068653 -0.033194  0.084968 -0.208254   \n",
       "6  0.050144 -0.443587  0.002821 -0.611987 -0.045575 -0.219633 -0.167716   \n",
       "7  0.686133 -0.076127 -1.222127 -0.358222  0.324505 -0.156742  1.943465   \n",
       "8 -0.328783 -0.210077 -0.499768  0.118765  0.570328  0.052736 -0.073425   \n",
       "9  0.150219  0.739453 -0.540980  0.476677  0.451773  0.203711 -0.246914   \n",
       "\n",
       "        V22       V23       V24       V25       V26       V27       V28  \\\n",
       "0  0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053   \n",
       "1 -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   \n",
       "2  0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   \n",
       "3  0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   \n",
       "4  0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   \n",
       "5 -0.559825 -0.026398 -0.371427 -0.232794  0.105915  0.253844  0.081080   \n",
       "6 -0.270710 -0.154104 -0.780055  0.750137 -0.257237  0.034507  0.005168   \n",
       "7 -1.015455  0.057504 -0.649709 -0.415267 -0.051634 -1.206921 -1.085339   \n",
       "8 -0.268092 -0.204233  1.011592  0.373205 -0.384157  0.011747  0.142404   \n",
       "9 -0.633753 -0.120794 -0.385050 -0.069733  0.094199  0.246219  0.083076   \n",
       "\n",
       "   Amount  Class  \n",
       "0  149.62      0  \n",
       "1    2.69      0  \n",
       "2  378.66      0  \n",
       "3  123.50      0  \n",
       "4   69.99      0  \n",
       "5    3.67      0  \n",
       "6    4.99      0  \n",
       "7   40.80      0  \n",
       "8   93.20      0  \n",
       "9    3.68      0  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop('Class',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=pd.Series(df['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=100, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators='warn', n_jobs=None,\n",
       "                                              oob_score=False, random_state=100,\n",
       "                                              verbose=0, warm_start=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid=[{'max_depth': array([4, 5, 6]),\n",
       "                          'max_features': array([3, 4]),\n",
       "                          'n_estimators': [10, 15]}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = [{'n_estimators': [10, 15],\n",
    "'max_features': np.arange(3, 5),\n",
    "'max_depth': np.arange(4, 7)}]\n",
    "clf=GridSearchCV(estimator=RandomForestClassifier(random_state=100),param_grid=parameters,scoring='roc_auc',cv=3)\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 6, 'max_features': 3, 'n_estimators': 15}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba=clf.predict_proba(x_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUC=roc_auc_score(y_test,y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9462664156037156"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
